# Inference service configuration.
service:
  host: "0.0.0.0"
  port: 8000

model:
  registry_uri: "file:./mlruns"
  model_name: "finance-math-regressor"
  model_stage: "Production"
  model_uri: runs:/5d230e940f6749b4af7ad9783d673be4/model

logging:
  level: "INFO"
